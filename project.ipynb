{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "124e5672",
   "metadata": {},
   "source": [
    "# Custom Chatbot Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4a94b3",
   "metadata": {},
   "source": [
    "The goal of this project is to build a personal chatbot that can help in finding Machine Learning research papers. The dataset that I have chosen is the Arxiv dataset, which contains research papers from various fields. Arxiv is the perfect dataset for this task, as it has served the public and research communities by providing open access to scholarly articles, from the vast branches of physics to the many subdisciplines of computer science to everything in between, including math, statistics, electrical engineering, quantitative biology, and economics. \n",
    "As the dataset is too large, it'll be filtered according to the following criteria:\n",
    "* It will be filtered to include only the papers related to Machine Learning.\n",
    "* It will be filtered to include only the papers published in the last 5 days. (The last 5 years contain >422538 ML related papers, and the cost limit is obviously 2-3 USD).\n",
    "* As only the abstracts are required for a recommendation, only the abstracts will be used.\n",
    "\n",
    "The gpt3.5 model's training data was cut-off in Sep 2021. The chosen dataset is suitable as it contains the latest papers (between 2024-12-10 to 2024-12-12), and the model would not have seen them before. This can help us verify the model's ability to generate relevant responses based on the new data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a63d4c5f",
   "metadata": {},
   "source": [
    "## Data Wrangling\n",
    "\n",
    "TODO: In the cells below, load your chosen dataset into a `pandas` dataframe with a column named `\"text\"`. This column should contain all of your text data, separated into at least 20 rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69b83a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1007102</th>\n",
       "      <td>2401.01987</td>\n",
       "      <td>Title: Representation Learning of Multivariate...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1008464</th>\n",
       "      <td>2401.03349</td>\n",
       "      <td>Title: Image Inpainting via Tractable Steering...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1014389</th>\n",
       "      <td>2401.09274</td>\n",
       "      <td>Title: Avoiding strict saddle points of noncon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1014697</th>\n",
       "      <td>2401.09582</td>\n",
       "      <td>Title: eipy: An Open-Source Python Package for...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1016754</th>\n",
       "      <td>2401.11641</td>\n",
       "      <td>Title: Revolutionizing Finance with LLMs: An O...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                               text\n",
       "1007102  2401.01987  Title: Representation Learning of Multivariate...\n",
       "1008464  2401.03349  Title: Image Inpainting via Tractable Steering...\n",
       "1014389  2401.09274  Title: Avoiding strict saddle points of noncon...\n",
       "1014697  2401.09582  Title: eipy: An Open-Source Python Package for...\n",
       "1016754  2401.11641  Title: Revolutionizing Finance with LLMs: An O..."
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "arxiv_cats = json.load(open('arxiv_cats.json'))\n",
    "df = pd.read_json('last_5_yr_data.jsonl', lines=True)\n",
    "ml_cats = ['cs.CL', 'cs.LG', 'stat.ML']\n",
    "df = df[df['categories'].apply(lambda x: any(cat in x for cat in ml_cats))]\n",
    "\n",
    "\n",
    "df['update_date'] = pd.to_datetime(df['update_date'])\n",
    "filter_date = pd.to_datetime('2024-12-10')\n",
    "df = df[df['update_date'] > filter_date]\n",
    "df = df[df.id.str.startswith('24')]\n",
    "df['categories'] = df.categories.apply(lambda x: x.split()).apply(lambda x: [arxiv_cats[cat] for cat in x])\n",
    "df = df.loc[:, ['id', 'title', 'authors' ,'categories', 'abstract']]\n",
    "df['text'] = df.apply(lambda x: f'Title: {x[\"title\"]}\\nAuthors: {x[\"authors\"]}\\nCategories:{\",\".join(x[\"categories\"])}\\nAbstract:\\n {x[\"abstract\"][:250]}', axis=1)\n",
    "df.drop(columns=['title', 'authors', 'categories', 'abstract'], inplace=True)\n",
    "df.to_csv('2024-12-10_ml_papers.csv', index=False)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "825d4499",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(983, 2)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf2b725",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting openai embeddings for the text\n",
    "import openai\n",
    "apikey = \"YOUR_API_KEY\"\n",
    "openai.api_base = \"https://openai.vocareum.com/v1\"\n",
    "openai.api_key = apikey\n",
    "\n",
    "EMBEDDING_MODEL_NAME = \"text-embedding-3-small\"\n",
    "batch_size = 110"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "acb3a9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = []\n",
    "for i in range(0, len(df), batch_size):\n",
    "    try:\n",
    "        response = openai.Embedding.create(\n",
    "            input=df.iloc[i:i+batch_size][\"text\"].tolist(),\n",
    "            engine=EMBEDDING_MODEL_NAME\n",
    "        )\n",
    "        embeddings.extend([data[\"embedding\"] for data in response[\"data\"]])\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "# Add embeddings to DataFrame\n",
    "df[\"embedding\"] = embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "63aa5f89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Title: Representation Learning of Multivariate Time Series using Attention and\\n  Adversarial Training\\nAuthors: Leon Scharw\\\\\"achter and Sebastian Otte\\nCategories:Machine Learning\\nAbstract:\\n   A critical factor in trustworthy machine learning is to develop robust\\nrepresentations of the training data. Only under this guarantee methods are\\nlegitimate to artificially generate data, for example, to counteract imbalanced\\ndatasets or provide c'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.reset_index(drop=True, inplace=True)\n",
    "df.loc[0, 'text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1801dd8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('2024-12-10_ml_papers.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae769871",
   "metadata": {},
   "source": [
    "## Custom Query Completion\n",
    "\n",
    "TODO: In the cells below, compose a custom query using your chosen dataset and retrieve results from an OpenAI `Completion` model. You may copy and paste any useful code from the course materials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "582f0656",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2401.01987</td>\n",
       "      <td>Title: Representation Learning of Multivariate...</td>\n",
       "      <td>[0.018088510259985924, -0.0009138910681940615,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2401.03349</td>\n",
       "      <td>Title: Image Inpainting via Tractable Steering...</td>\n",
       "      <td>[0.005470732226967812, 0.01446774136275053, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2401.09274</td>\n",
       "      <td>Title: Avoiding strict saddle points of noncon...</td>\n",
       "      <td>[0.0009837907273322344, 0.008160199038684368, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2401.09582</td>\n",
       "      <td>Title: eipy: An Open-Source Python Package for...</td>\n",
       "      <td>[0.013271886855363846, -0.01609743945300579, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2401.11641</td>\n",
       "      <td>Title: Revolutionizing Finance with LLMs: An O...</td>\n",
       "      <td>[0.004357148893177509, 0.018598299473524094, 0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id                                               text  \\\n",
       "0  2401.01987  Title: Representation Learning of Multivariate...   \n",
       "1  2401.03349  Title: Image Inpainting via Tractable Steering...   \n",
       "2  2401.09274  Title: Avoiding strict saddle points of noncon...   \n",
       "3  2401.09582  Title: eipy: An Open-Source Python Package for...   \n",
       "4  2401.11641  Title: Revolutionizing Finance with LLMs: An O...   \n",
       "\n",
       "                                           embedding  \n",
       "0  [0.018088510259985924, -0.0009138910681940615,...  \n",
       "1  [0.005470732226967812, 0.01446774136275053, -0...  \n",
       "2  [0.0009837907273322344, 0.008160199038684368, ...  \n",
       "3  [0.013271886855363846, -0.01609743945300579, 0...  \n",
       "4  [0.004357148893177509, 0.018598299473524094, 0...  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tiktoken\n",
    "from openai.embeddings_utils import distances_from_embeddings\n",
    "\n",
    "#load the dataframe\n",
    "df = pd.read_csv('2024-12-10_ml_papers.csv')\n",
    "df.embedding = df.embedding.apply(eval).apply(np.array)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9dd534f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_prompt_embedding(prompt):\n",
    "    em_model = \"text-embedding-3-small\"\n",
    "    response = openai.Embedding.create(engine=em_model, input=prompt)\n",
    "    return np.array(response[\"data\"][0][\"embedding\"])\n",
    "\n",
    "def calculate_prompt_similarity(prompt, embedding_list):\n",
    "    prompt_embedding = get_prompt_embedding(prompt)\n",
    "    distances = distances_from_embeddings(prompt_embedding, embedding_list, distance_metric='cosine')\n",
    "    return distances\n",
    "\n",
    "def get_df_with_distances(prompt, context_df):\n",
    "    distances = calculate_prompt_similarity(prompt, context_df.embedding.tolist())\n",
    "    df_with_distances = context_df.copy()\n",
    "    df_with_distances['distance'] = distances\n",
    "    df_with_distances.sort_values(by='distance', inplace=True, ascending=True)\n",
    "    return df_with_distances\n",
    "\n",
    "def get_custom_prompt_text(user_prompt, context):\n",
    "    custom_prompt = f\"\"\"\n",
    "    Answer the question based on the context below, and if the\n",
    "    question can't be answered, say \"I don't know\"\n",
    "    Context: \n",
    "    {context}\n",
    "\n",
    "    ---\n",
    "    Question: {user_prompt}\n",
    "    Answer:\n",
    "    \"\"\"\n",
    "    return custom_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c403f543",
   "metadata": {},
   "outputs": [],
   "source": [
    "def populate_prompt_with_context(prompt, context_df):\n",
    "    tokenizer= tiktoken.get_encoding(\"cl100k_base\")\n",
    "    prompt_template = f\"\"\"\n",
    "    Answer the question based on the context below, and if the question\n",
    "    can't be answered based on the context, say \"I don't know\"\n",
    "\n",
    "    Context: \n",
    "\n",
    "    ---\n",
    "\n",
    "    Question: {prompt}\n",
    "    Answer:\"\"\"\n",
    "    curr_prompt_length = len(tokenizer.encode(prompt_template))\n",
    "    df_with_dist = get_df_with_distances(prompt, context_df)\n",
    "    contexts = []\n",
    "    for text in df_with_dist.text.values:\n",
    "        if len(tokenizer.encode(text)) + curr_prompt_length < 3700:\n",
    "            contexts.append(text)\n",
    "            curr_prompt_length += len(tokenizer.encode(text))\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    context = \"\\n\\n########\\n\\n\".join(contexts)\n",
    "    return get_custom_prompt_text(prompt, context)\n",
    "    \n",
    "def get_answer(input_prompt):\n",
    "    openai_response = openai.Completion.create(\n",
    "        model=\"gpt-3.5-turbo-instruct\",\n",
    "        prompt=input_prompt,\n",
    "        max_tokens=150\n",
    "    )\n",
    "    return openai_response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1783f146",
   "metadata": {},
   "source": [
    "## Custom Performance Demonstration\n",
    "\n",
    "TODO: In the cells below, demonstrate the performance of your custom query using at least 2 questions. For each question, show the answer from a basic `Completion` model query as well as the answer from your custom query."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f11fdc0",
   "metadata": {},
   "source": [
    "### Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4901c850",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\"Deep Learning for Finance: Applications and Opportunities\" by Arize Nwokoro and Huan Zhao (2020) discusses the potential of applying Large Language Models (LLMs) in various financial tasks such as sentiment analysis, market prediction, fraud detection, and portfolio management. The paper provides a comprehensive overview of LLMs, their capabilities, and challenges in the financial domain. It also explores various use cases and provides a roadmap for leveraging LLMs in finance. The authors highlight the potential benefits of using LLMs, including improved accuracy, cost reduction, and automation of tedious tasks. Additionally, the paper discusses the ethical implications of using LLMs in finance and suggests measures to address potential biases. Overall, this paper provides valuable\n"
     ]
    }
   ],
   "source": [
    "#Before context adding\n",
    "prompt = \"Suggest a recent paper on using LLMs in Finance.\"\n",
    "raw_response = get_answer(prompt)\n",
    "print(raw_response['choices'][0]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "bd7a093b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Revolutionizing Finance with LLMs: An Overview of Applications and Insights \n",
      "    by Huaqin Zhao, Zhengliang Liu, Zihao Wu, Yiwei Li, Tianze Yang, Peng Shu, Shaochen Xu, Haixing Dai, Lin Zhao, Hanqi Jiang, Yi Pan, Junhao Chen, Yifan Zhou, Gengchen Mai, Ninghao Liu, and Tianming Liu.\n"
     ]
    }
   ],
   "source": [
    "#after context adding\n",
    "prompt = \"Suggest a recent paper on using LLMs in Finance.\"\n",
    "custom_prompt = populate_prompt_with_context(prompt, df)\n",
    "response = get_answer(custom_prompt)\n",
    "print(response['choices'][0]['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e86e37c",
   "metadata": {},
   "source": [
    "### Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6f646989",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Yes, there are many recent papers on tokenizers. Here are a few examples:\n",
      "\n",
      "1. \"Tokenization Strategies for Neural Machine Translation\" (2020) by Jacob Devlin et al. This paper explores different tokenization strategies for neural machine translation and evaluates their effects on translation performance.\n",
      "\n",
      "2. \"Robust Tokenization via Pre-trained Word Piece Embeddings\" (2019) by Mingda Chen et al. This paper introduces a novel tokenization approach that uses pre-trained word piece embeddings to improve the robustness of tokenization in natural language processing tasks.\n",
      "\n",
      "3. \"Unicode-aware Tokenization for Natural Language Processing\" (2020) by Diederik P. Kingma et al. This paper proposes a novel Unicode-aware tokenization\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Are there any recent papers on tokenizers?\"\n",
    "raw_response = get_answer(prompt)\n",
    "print(raw_response['choices'][0]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "11c07a54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Title: One Token Can Help! Learning Scalable and Pluggable Virtual Tokens for\n",
      "  Retrieval-Augmented Large Language Models\n",
      "Authors: Yutao Zhu, Zhaoheng Huang, Zhicheng Dou, Ji-Rong Wen\n",
      "Categories:Computation and Language\n",
      "Abstract:\n",
      "   Retrieval-augmented generation (RAG) is a promising way to improve large\n",
      "language models (LLMs) for generating more factual, accurate, and up-to-date\n",
      "content. Existing methods either optimize prompts to guide LLMs in leveraging\n",
      "retrieved informatio\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Are there any recent papers on tokenizers?\"\n",
    "custom_prompt = populate_prompt_with_context(prompt, df)\n",
    "response = get_answer(custom_prompt)\n",
    "print(response['choices'][0]['text'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
